---
layout: article
title: 1. 선형 회귀 분석
permalink: /notes/kr/note_home/1.linearReg
key: notes
sidebar:
  nav: notes-kr
aside:
  toc: true
mermaid: true
---

# 선형 회귀 분석
## 선형회귀의 기본 개념
선형회귀는 대표적인 알고리즘이다. 단순하면서도 유용하고, 다른 알고리즘의 기반이되기 때문에 완벽한 이해가 필요하다.
지도학습으로, 값을 찾아나가는 회귀유형이다.
종속 변수 하나와 독립(설명) 변수들 간의 관계를 수식으로 설명/예측하는 방법 


- x가 1일 때, y값은 3
- x가 2일 때, y값은 5
- x가 3일 때, y값은 7 


x의 값으로 4가 들어왔을 때, y값은 무엇일까?
y가 함수라고 가정했을 때, f(x) = 2x + 1라고 유추할 수 있다. 따라서, x가 4일 때는 9라는 y값을 가질 것이라고 생각할 것이다.

이러한 문제는 컴퓨터는 어떻게 유추할 수 있을까? 바로 선형 회귀분석(Linear Regression)을 통해서이다. x에 대한 y값을 알고 있을 때, 새로운 x값이 주어졌을 때 그에 대한 y값을 유추하는 것이다.


조금 쉽게 선형회귀가 쓰이는 예시를 들어보자, 집 값을 유추한다고 했을 때
- 10평일 때 1억,
- 20평일 때 5억,
- 30평일 때 9억 이라고 가정해보자
집의 평수는 x, 집 값은 y이다. 그렇다면, 40평이라는 x값이 주어졌을 때, 집 값은 얼마일까? 바로 y를 찾는 것이다.


## 선형 회귀 용어

좀전의 예시를 함수로 표현하자면 다음과 같이 표현할 수 있다.
y = a + bx , (a, b는 상수)

통계에서는 x: 독립(설명) 변수, y: 종속 변수로도 표현을 하고

- 목표 변수(target variable, output variable): 맞추려고 하는 값
- 입력 변수(input variable, feature): 맞추기 위해 사용하는 값

즉, x라는 속성을 통해 y의 값을 찾는 것이다
- x = 집 평수, y = 집 값
- x = 공부량, y = 성적
- x = 아빠 키, y = 아들 키
으로 표현할 수 있다

선형회귀는 지도학습으로 미리 데이터를 '학습'시킨다. 학습시킨 데이터에서 우리는 새로운 x값이 주어졌을 때, 가장 최적의 y를 찾아야 한다. 즉, 가지고 있는 데이터를 가장 잘 대변해주는 선, 최적선을 찾는 것이다. 


## 가설함수
어떤 선이 가장 집값을 잘 대변해줄 수 있는 선이라고 할 수 있을까?  A일까? B일까?

![Alt text](img/linear1.jpg)


우리는 이렇게, 어떤 선이 최적의 선인지 알기 위해서 여러 선을 시험해볼 것이다. 시도하는 이 함수 하나하나를 가설함수(Hypothesis function)이라 부른다.
우리는 곡선이 아니라 linear! 선이기 때문에 1차함수가 되며 아까 2에서 언급하였던 y = a + bx로 표현할 수 있는 것이다.

결국, 선형 회귀의 입문은 계수 a와 b를 찾아내는 것이다
우리가 y=ax+b라고 표현을 하지만, 회귀에서 가설함수로 표현하자면 
$$
h(x) = \theta_{0} + \theta_{1}x
$$
로 표현할 수 있다. 입력변수 x 에 대한 함수이며, a와 b 대신 theta를 써서 표현한다. 


Q. 왜 a와 b를 쓰지 않는 것일까..?
사실, 집 값, 키, 성적을 유추한다고 하였을 때, 한 가지 요소만이 영향을 미치는 것이 아닐 것이다. 집 값에는 평수 뿐만 아니라 위치, 층수, 연식 등이 있을 것이다. 키(height) 또한 아빠의 키 뿐만 아니라 아이의 수면, 영양 등이 영향을 미칠 것일텐데 모든 것을 a, b, c 로 표현하면 너무 많고 복잡하기 때문에 $$ theta $$를 써서 표현한다.


$$
h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \ldots
$$



Tip. h아래 theta값을 주어, h는 세타값에 따라 결정된다고 표현할 수 있다
$$
h_{theta}(x) = \theta_{0} + \theta_{1}x
$$


<!-- 이미지 ex -->
<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQW0Z94iqO01RBz7uaesVFC5hG-J4y-ldNCHg&usqp=CAU" width="100">


## 평균 제곱 오차(MSE)
평균 제곱 오차(Mean Squared Error)는 데이터와 가설함수가 얼마나 떨어져있는지를 나타내는 표현이다. 즉, 각 데이터의 실제 값과 데이터가 예측하는 값이 차이가 난다. 
![Alt text](img/linear2_mse.jpg)

최적선은 A인 것 같은데, A라고 가설을 세웠지만 실제 값들은 선들과 일치하지 않고 떨어져 있는 경우들이 있다. 이러한 경우 예측값에서 실제값을 빼게 되는데, 1.에서 예로 들었던 것과 같이 x=20평일 때 y=5억 이라는 데이터를 학습시켰으나, 선형 최적선은 20평일 때 4.5억으로 예측을 한다면 -5천만원이라는 오차가 생기게 된다. 추가적으로 30평일 때 8억이라고 예측했으나, 9억이라는 데이터를 가지고 있다면 +1억이라는 오차가 생기게 된다. 

이렇게 오차들을 제곱하여 더해서 데이터 수(n)개로 나눈 값을 평균 제곱 오차라고 한다. 데이터가 많겠지만 2개만으로 예시를 든다면

$$
MSE = \frac{{(-5)^2 + 1^2}}{2}
$$

이런 값이 된다. 
즉, MSE가 크다는 것은 실제 데이터와 예측값의 오차가 크다는 것이고, 가설 함수는 데이터를 잘 표현하지 못한 것이라 할 수 있다.


비용(cost)에 대해 오차가 음수일 때와 양수일 때가 있기 때문에 동일하게 간주해야하기 때문이다. 그런데! 절대값이 아닌 제곱을 이용하는 이유는, 가설이 잘못되었을 경우 패널티를 더욱 강하게 주기 때문에, 학습에 효율적이라고 여긴다. 컴퓨터의 입장에서는 연산속도가 느려질 수 있기 때문에 최소제곱법을 사용하여 오차의 제곱을 더해주는 것이다.




##평균 제곱 오차의 일반화
